<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Brigitte Bigi">
  <title>SPPAS Documentation</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="etc/styles/sppas.css">
      <link rel="icon" href="./etc/icons/sppas.png" />
  
      <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-38055333-4']);
        _gaq.push(['_trackPageview']);
      
        (function() {
          var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
          ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
          var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
      </script>
</head>
<body>

    <!----------------------------------------------------------------->

    <div class="header">
        <div class="title">
            <div class="h1title">SPPAS</div>
            <div class="h2title">the automatic annotation and analysis of speech</div>
            <div class="h3title">a software tool by Brigitte Bigi, researcher at</div>
            <div class="h3title">Laboratoire Parole et Langage - Aix-en-Provence - France</div>
        </div>
    </div>

    <!----------------------------------------------------------------->

    <div class="menu">
        <a class="home"     href="index.html">Home</a>
        <a class="install"  href="installation.html">Installation</a>
        <a class="download" href="download.html">Download</a>
        <a class="tutorial" href="tutorial.html">Tutorials</a>
        <a class="documentation" href="documentation.html">Documentation</a>
        <a class="group"    href="https://groups.google.com/forum/?fromgroups#!forum/sppas-users">User's group</a>
    </div>

    <!----------------------------------------------------------------->
<header>
<h1 class="title">SPPAS Documentation</h1>
<h2 class="author">Brigitte Bigi</h2>
<h3 class="date">Version 1.8.0</h3>
</header>
<nav id="TOC">
<ul>
<li><a href="#automatic-annotations">Automatic Annotations</a><ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#about-this-chapter">About this chapter</a></li>
<li><a href="#annotations-methodology">Annotations methodology</a></li>
<li><a href="#file-formats-and-tier-names">File formats and tier names</a></li>
<li><a href="#recorded-speech">Recorded speech</a></li>
<li><a href="#orthographic-transcription">Orthographic Transcription</a></li>
<li><a href="#automatic-annotations-with-gui">Automatic Annotations with GUI</a></li>
<li><a href="#automatic-annotations-with-cli">Automatic Annotations with CLI</a></li>
<li><a href="#the-procedure-outcome-report">The procedure outcome report</a></li>
</ul></li>
<li><a href="#inter-pausal-units-ipus-segmentation">Inter-Pausal Units (IPUs) segmentation</a><ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#silencespeech-segmentation">Silence/Speech segmentation</a></li>
<li><a href="#silencespeech-segmentation-time-aligned-with-a-transcription">Silence/Speech segmentation time-aligned with a transcription</a></li>
<li><a href="#split-into-multiple-files">Split into multiple files</a></li>
<li><a href="#perform-ipus-segmentation-with-the-gui">Perform IPUs Segmentation with the GUI</a></li>
<li><a href="#perform-ipus-segmentation-with-the-cli">Perform IPUs Segmentation with the CLI</a></li>
</ul></li>
<li><a href="#tokenization">Tokenization</a><ul>
<li><a href="#overview-1">Overview</a></li>
<li><a href="#adapt-tokenization">Adapt Tokenization</a></li>
<li><a href="#perform-tokenization-with-the-gui">Perform Tokenization with the GUI</a></li>
<li><a href="#perform-tokenization-with-the-cli">Perform Tokenization with the CLI</a></li>
</ul></li>
<li><a href="#phonetization">Phonetization</a><ul>
<li><a href="#overview-2">Overview</a></li>
<li><a href="#adapt-phonetization">Adapt Phonetization</a></li>
<li><a href="#perform-phonetization-with-the-gui">Perform Phonetization with the GUI</a></li>
<li><a href="#perform-phonetization-with-the-cli">Perform Phonetization with the CLI</a></li>
</ul></li>
<li><a href="#alignment">Alignment</a><ul>
<li><a href="#overview-3">Overview</a></li>
<li><a href="#adapt-alignment">Adapt Alignment</a></li>
<li><a href="#perform-alignment-with-the-gui">Perform Alignment with the GUI</a></li>
<li><a href="#perform-alignment-with-the-cli">Perform Alignment with the CLI</a></li>
</ul></li>
<li><a href="#syllabification">Syllabification</a><ul>
<li><a href="#overview-4">Overview</a></li>
<li><a href="#adapt-syllabification">Adapt Syllabification</a></li>
<li><a href="#perform-syllabification-with-the-gui">Perform Syllabification with the GUI</a></li>
<li><a href="#perform-syllabification-with-the-cli">Perform Syllabification with the CLI</a></li>
</ul></li>
<li><a href="#repetitions">Repetitions</a><ul>
<li><a href="#overview-5">Overview</a></li>
<li><a href="#perform-repetitions-with-the-gui">Perform Repetitions with the GUI</a></li>
<li><a href="#perform-repetitions-with-the-cli">Perform Repetitions with the CLI</a></li>
</ul></li>
<li><a href="#momel-and-intsint">Momel and INTSINT</a><ul>
<li><a href="#momel-modelling-melody">Momel (modelling melody)</a></li>
<li><a href="#encoding-of-f0-target-points-using-the-intsint-system">Encoding of F0 target points using the &quot;INTSINT&quot; system</a></li>
<li><a href="#perform-momel-and-intsint-with-the-gui">Perform Momel and INTSINT with the GUI</a></li>
<li><a href="#perform-momel-and-intsint-with-the-cli">Perform Momel and INTSINT with the CLI</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<hr />
<h1 id="automatic-annotations"><a href="#automatic-annotations">Automatic Annotations</a></h1>
<h2 id="introduction"><a href="#introduction">Introduction</a></h2>
<h3 id="about-this-chapter"><a href="#about-this-chapter">About this chapter</a></h3>
<p>This chapter is not a description on how each automatic annotation is implemented and how it's working: the references are available for that specific purpose!</p>
<p>Instead, this chapter describes how each automatic annotation can be used in SPPAS, i.e. what is the goal of the annotation, what are the requirements, what kind of resources are used, what is the expected result and how to perform it with SPPAS. Each automatic annotation is illustrated as a workflow schema, where:</p>
<ul>
<li>blue boxes represent the name of the automatic annotation,</li>
<li>red boxes represent tiers (with their name mentioned in white),</li>
<li>green boxes indicate the resource,</li>
<li>yellow boxes indicate the annotated file (given as input or produced as output).</li>
</ul>
<h3 id="annotations-methodology"><a href="#annotations-methodology">Annotations methodology</a></h3>
<p>The kind of process to implement in the perspective of obtaining rich and broad-coverage multimodal/multi-levels annotations of a corpus is illustrated in next Figure. It describes each step of the annotation workflow. This Figure must be read from top to bottom and from left to right, starting by the recordings and ending to the analysis of annotated files. Yellow boxes represent manual annotations, blue boxes represent automatic ones.</p>
<figure>
<img src="./etc/figures/methodo.png" alt="Annotation methodology" /><figcaption>Annotation methodology</figcaption>
</figure>
<p>After the recording, the first annotation to perform is IPUs segmentation. Indeed, at a first stage, the audio signal must be automatically segmented into Inter-Pausal Units (IPUs) which are blocks of speech bounded by silent pauses of more than X ms, and time-aligned on the speech signal. An orthographic transcription has to be performed manually inside the set of IPUs. Then tokenization will normalize the transcription. The phonetization process will convert the normalized text in a set of pronunciations using X-SAMPA standard. Alignment will perform segmentation at phonemes and tokens levels, etc.</p>
<p>At the end of each automatic annotation process, SPPAS produces a Procedure Outcome Report that contains important information about the annotations.</p>
<p>Among others, SPPAS is able to produce automatically annotations from a recorded speech sound and its orthographic transcription. Let us first introduce what is required in terms of files, and what is the exactly meaning of &quot;recorded speech&quot; and &quot;orthographic transcription&quot;.</p>
<h3 id="file-formats-and-tier-names"><a href="#file-formats-and-tier-names">File formats and tier names</a></h3>
<p>When using the Graphical User Interface, the file format for input and output can be fixed in the Settings and is applied to all annotations, however file names of each annotation is already fixed and can't be changed.</p>
<p>When using the Command-Line interface, or when using scripts, each annotation can be configured independently (file format and file names).</p>
<p>In all cases, <strong>the name of the tiers are fixed and can't be changed</strong>!</p>
<h3 id="recorded-speech"><a href="#recorded-speech">Recorded speech</a></h3>
<p>First of all:</p>
<blockquote>
<p>Only <code>wav</code>, <code>aiff</code> and <code>au</code> audio files and only as mono are supported by SPPAS.</p>
</blockquote>
<p>SPPAS verifies if the audio file is 16 bits and 16000 Hz sample rate. Otherwise it automatically converts to this configuration. For very long files, this may take time. So, the following are possible:</p>
<ol type="1">
<li>be patient;</li>
<li>prepare by your own the required wav/mono/16000Hz/16bits files to be used in SPPAS.</li>
</ol>
<p>Secondly, a relatively good recording quality is expected. Providing a guideline or recommendation for that is impossible, because it depends: &quot;IPU segmentation&quot; requires a better quality compared to what is expected by &quot;Alignment&quot;, and for that latter, it depends on the language.</p>
<figure>
<img src="./etc/screenshots/signal.png" alt="Example of recorded speech" /><figcaption>Example of recorded speech</figcaption>
</figure>
<h3 id="orthographic-transcription"><a href="#orthographic-transcription">Orthographic Transcription</a></h3>
<p>An orthographic transcription is often the minimum requirement for a speech corpus so it is at the top of the annotation procedure, and it is the entry point for most of the automatic annotations, including automatic speech segmentation. But clearly, there are different ways to pronounce the same utterance. Consequently, when a speech corpus is transcribed into a written text, the transcriber is immediately confronted with the following question: how to reflect the orality of the corpus? <em>Transcription conventions</em> are then designed to provide rules for writing speech corpora. These conventions establish phenomena to transcribe and also how to annotate them.</p>
<p>In that sense, the orthographic transcription is a subjective representation of what is &quot;perceived&quot; in the signal. It <strong>must</strong> includes:</p>
<ul>
<li>filled pauses;</li>
<li>short pauses;</li>
<li>repeats;</li>
<li>noises and laugh items (not available for: English, Japanese and Cantonese).</li>
</ul>
<p>In speech, many phonetic variations occur. Some of these phonologically known variants are predictable and can be included in the pronunciation dictionary but many others are still unpredictable like invented words, regional words or words borrowed from another language. These specifics have a direct consequence on the automatic phonetization procedure as shown in (Bigi 2012). As a consequence, from the beginning of its development it was considered to be essential for SPPAS to deal with <strong>Enriched Orthographic Transcriptions</strong>.</p>
<p>The transcription must use the following convention to represent speech phenomena. All the symbols must be surrounded by whitespace.</p>
<ul>
<li>truncated words, noted as a '-' at the end of the token string (an ex- example);</li>
<li>noises, noted by a '*' (not available for: English, Japanese and Cantonese);</li>
<li>laughs, noted by a '@' (not available for: English, Japanese and Cantonese);</li>
<li>short pauses, noted by a '+';</li>
<li>elisions, mentioned in parenthesis;</li>
<li>specific pronunciations, noted with brackets [example,eczap];</li>
<li>comments are noted inside braces or brackets without using comma {this} or [this and this];</li>
<li>liaisons, noted between '=' (an =n= example);</li>
<li>morphological variants with &lt;like,lie ok&gt;,</li>
<li>proper name annotation, like $ John S. Doe $.</li>
</ul>
<p>SPPAS also allows to include in the transcription:</p>
<ul>
<li>regular punctuations,</li>
<li>numbers: they will be automatically converted to their written form.</li>
</ul>
<p>This convention is not software-dependent which means that it can be performed with IPUscriber tool of SPPAS, Praat, Annotation Pro, ...</p>
<p>The result is what we call an Enriched Orthographic construction, from which two derived transcriptions can be generated automatically:</p>
<ol type="1">
<li>the <strong>standard transcription</strong> is the list of orthographic tokens</li>
<li>a specific transcription from which the phonetic tokens are obtained to be used by the grapheme-phoneme converter that is named <strong>faked transcription</strong>.</li>
</ol>
<p>As for example with the following sentence:</p>
<p><em>This is + hum... an enrich(ed) transcription {loud} number 1!</em></p>
<p>The derived transcriptions are:</p>
<ul>
<li>standard: this is hum an enriched transcription number one</li>
<li>faked: this is + hum an enrich transcription number one</li>
</ul>
<h3 id="automatic-annotations-with-gui"><a href="#automatic-annotations-with-gui">Automatic Annotations with GUI</a></h3>
<p>To perform automatic annotations with SPPAS Graphical User Interface, select first the list of audio files and/or a directory, then click on the &quot;Annotate&quot; button.</p>
<figure>
<img src="./etc/screenshots/AAP.png" alt="the annotate panel" /><figcaption>the annotate panel</figcaption>
</figure>
<ol type="1">
<li><p>Enable each annotation to perform by clicking on the button in red. It will be turned green.</p></li>
<li><p>Each annotation can be configured by clicking on the &quot;Configure...&quot; text in blue.</p></li>
<li><p>Select the language for all annotations in one, or for each one independently by clicking on the &quot;chains&quot; button.</p></li>
<li><p>Click on the <em>Perform annotations</em> button... and wait! Please, be patient! Particularly for Tokenization or Phonetization: loading resources (lexicons or dictionaries) is very long. Sometimes, the progress bar does not really &quot;progress&quot;... it depends on the system. So, just wait!</p></li>
<li><p>It is important to read the Procedure Outcome report to check that everything happened normally during the automatic annotations.</p></li>
</ol>
<h3 id="automatic-annotations-with-cli"><a href="#automatic-annotations-with-cli">Automatic Annotations with CLI</a></h3>
<p>To perform automatic annotations with SPPAS Command-line User Interface, there is a main program <code>annotation.py</code> and each annotation has also its own program with more options than the main one.</p>
<p>This main program performs automatic annotations on a given file or on all files of a directory. It strictly corresponds to the button <code>Perform annotations</code> of the GUI. All annotations are pre-configured: no specific option can be specified. This configuration is fixed in the source code packages, via a file with extension <code>.cfg</code> for each annotation.</p>
<pre><code>usage: annotation.py -w file|folder [options]</code></pre>
<pre><code>optional arguments:
   -h, --help      show this help message and exit
   -w file|folder  Input wav file name, or folder
   -l lang         Input language, using iso639-3 code
   -e extension    Output extension. One of: xra, textgrid, eaf, csv, ...
   --momel         Activate Momel and INTSINT
   --ipu           Activate IPUs Segmentation
   --tok           Activate Tokenization
   --phon          Activate Phonetization
   --align         Activate Alignment
   --syll          Activate Syllabification
   --rep           Activate Repetitions
   --all           Activate ALL automatic annotations
   --merge         Create a merged TextGrid file, if more than two automatic
                   annotations (this is the default).
   --nomerge       Do not create a merged TextGrid file.</code></pre>
<p>Examples of use:</p>
<pre><code>./sppas/bin/annotation.py ./samples/samples-eng
                    -l eng
                    --ipu --tok --phon --align</code></pre>
<p>A progress bar is displayed for each annotation. At the end of the process, a message indicates the name of the procedure outcome report file, which is <code>./samples/samples-eng.log</code> in our example. This file can be opened with any text editor (as Notepad++, vim, TextEdit, ...).</p>
<figure>
<img src="./etc/screenshots/CLI-example.png" alt="CLI: annotation.py output example" /><figcaption>CLI: annotation.py output example</figcaption>
</figure>
<h3 id="the-procedure-outcome-report"><a href="#the-procedure-outcome-report">The procedure outcome report</a></h3>
<p>It is very important to read conscientiously this report: it mentions exactly what happened during the automatic annotation process. This text can be saved: it is recommended to be kept it with the related data because it contains information that are interesting to know for anyone using the annotations!</p>
<p>The text first indicates the version of SPPAS that was used. This information is very important, mainly because annotations in SPPAS and their related resources are regularly improved and then, the result of the automatic process can change from one version to the other one.</p>
<p>Secondly, the text mentions information related to the given input:</p>
<ol type="1">
<li>the selected language of each annotation, only if the annotation is language-dependent). For some language-dependent annotations, SPPAS can still perform the annotation even if the resources for a given language are not available: in that case, select &quot;und&quot;, which is the iso639-3 code for &quot;undetermined&quot;.</li>
<li>the list of files to be annotated;</li>
<li>the list of annotations and if each annotation was activated or disabled. In that case, activated means that the checkbox of the AAP was checked by the user and that the resources are available for the given language. On the contrary, disabled means that either the checkbox of the AAP was not checked or the required resources are not available.</li>
</ol>
<p>In the following, each automatic annotation is described in details, for each annotated file. Four levels of information must draw your attention:</p>
<ol type="1">
<li>&quot;[ OK ]&quot; means that everything happened normally. The annotation was performed successfully.</li>
<li>&quot;[ IGNORED ]&quot; means that SPPAS ignored the file and didn't do anything.</li>
<li>&quot;[ WARNING ]&quot; means that something happened abnormally, but SPPAS found a solution, and the annotation was still performed successfully.</li>
<li>&quot;[ ERROR ]&quot; means that something happened abnormally and SPPAS failed to found a solution. The annotation was either not performed, or performed with a bad result.</li>
</ol>
<p>Finally, the &quot;Result statistics&quot; section mentions the number of files that was annotated for each step, or -1 if the annotation was disabled.</p>
<figure>
<img src="./etc/screenshots/log.png" alt="Procedure outcome report" /><figcaption>Procedure outcome report</figcaption>
</figure>
<h2 id="inter-pausal-units-ipus-segmentation"><a href="#inter-pausal-units-ipus-segmentation">Inter-Pausal Units (IPUs) segmentation</a></h2>
<h3 id="overview"><a href="#overview">Overview</a></h3>
<p>After recording, the first annotation to perform is IPUs segmentation. Indeed, at a first stage, the audio signal must be automatically segmented in Inter-Pausal Units (IPUs) which are blocks of speech bounded by silent pauses of more than X ms. This X duration depends on the language and it is commonly set to 200ms for French and 250ms for English. IPUs are time-aligned on the speech signal. This segmentation should be verified manually, depending on the quality of the recording: the better quality, thus the better IPUs segmentation.</p>
<p>The &quot;IPUs segmentation&quot; automatic annotation can perform 3 actions:</p>
<ol type="1">
<li>find silence/speech segmentation of a recorded file,</li>
<li>find silence/speech segmentation of a recorded file including the time-alignment of utterances of a transcription file,</li>
<li>split/save a recorded file into multiple files, depending on segments indicated in a time-aligned file.</li>
</ol>
<figure>
<img src="./etc/figures/segmworkflow.bmp" alt="IPU Segmentation workflow" /><figcaption>IPU Segmentation workflow</figcaption>
</figure>
<h3 id="silencespeech-segmentation"><a href="#silencespeech-segmentation">Silence/Speech segmentation</a></h3>
<p>The IPUs Segmentation annotation performs a silence detection from a recorded file. This segmentation provides an annotated file with one tier named &quot;IPU&quot;. The silence intervals are labelled with the &quot;#&quot; symbol, as speech intervals are labelled with &quot;ipu_&quot; followed by the IPU number.</p>
<p>The following parameters must be fixed:</p>
<ul>
<li><p>Minimum volume value (in seconds): If this value is set to zero, the minimum volume is automatically adjusted for each sound file. Try with it first, then if the automatic value is not correct, fix it manually. The Procedure Outcome Report indicates the value the system choose. The SndRoamer component can also be of great help: it indicates min, max and mean volume values of the sound.</p></li>
<li><p>Minimum silence duration (in seconds): By default, this is fixed to 0.2 sec., an appropriate value for French. This value should be at least 0.25 sec. for English.</p></li>
<li><p>Minimum speech duration (in seconds): By default, this value is fixed to 0.3 sec. The most relevent value depends on the speech style: for isolated sentences, probably 0.5 sec should be better, but it should be about 0.1 sec for spontaneous speech.</p></li>
<li><p>Speech boundary shift (in seconds): a duration which is systematically added to speech boundaries, to enlarge the speech interval.</p></li>
</ul>
<p>The procedure outcome report indicates the values (volume, minimum durations) that was used by the system for each sound file given as input. It also mentions the name of the output file (the resulting file). The file format can be fixed in the Settings of SPPAS (xra, TextGrid, eaf, ...).</p>
<figure>
<img src="./etc/screenshots/ipu-seg-log.png" alt="Procedure outcome report of IPUs Segmentation" /><figcaption>Procedure outcome report of IPUs Segmentation</figcaption>
</figure>
<p>The annotated file can be checked manually (preferably in Praat than Elan nor Anvil). If such values was not correct, then, delete the annotated file that was previously created, change the default values and re-annotate.</p>
<figure>
<img src="./etc/screenshots/ipu-seg-result1.png" alt="Result of IPUs Segmentation: silence detection" /><figcaption>Result of IPUs Segmentation: silence detection</figcaption>
</figure>
<p>Notice that the speech segments can be transcribed using the &quot;IPUScribe&quot; component.</p>
<figure>
<img src="./etc/screenshots/IPUscribe-2.png" alt="Orthographic transcription based on IPUs Segmentation" /><figcaption>Orthographic transcription based on IPUs Segmentation</figcaption>
</figure>
<h3 id="silencespeech-segmentation-time-aligned-with-a-transcription"><a href="#silencespeech-segmentation-time-aligned-with-a-transcription">Silence/Speech segmentation time-aligned with a transcription</a></h3>
<p>Inter-Pausal Units segmentation can also consist in aligning macro-units of a document with the corresponding sound.</p>
<p>SPPAS identifies silent pauses in the signal and attempts to align them with the inter-pausal units proposed in the transcription file, under the assumption that each such unit is separated by a silent pause. This algorithm is language-independent: it can work on any language.</p>
<p>In the transcription file, <strong>silent pauses must be indicated</strong> using both solutions, which can be combined:</p>
<ul>
<li>with the symbol '#',</li>
<li>with newlines.</li>
</ul>
<p>A recorded speech file must strictly correspond to a transcription, except for the extension expected as .txt for this latter. The segmentation provides an annotated file with one tier named &quot;IPU&quot;. The silence intervals are labelled with the &quot;#&quot; symbol, as speech intervals are labelled with &quot;ipu_&quot; followed by the IPU number then the corresponding transcription.</p>
<p>The same parameters than those indicated in the previous section must be fixed.</p>
<pre><code>Important:
This segmentation was tested on documents no longer than one paragraph
(about 1 minute speech).</code></pre>
<figure>
<img src="./etc/screenshots/ipu-seg-result2.png" alt="Silence/Speech segmentation with orthographic transcription" /><figcaption>Silence/Speech segmentation with orthographic transcription</figcaption>
</figure>
<h3 id="split-into-multiple-files"><a href="#split-into-multiple-files">Split into multiple files</a></h3>
<p>IPU segmentation can split the sound into multiple files (one per IPU), and it creates a text file for each of the tracks. The output file names are &quot;track_0001&quot;, &quot;track_0002&quot;, etc.</p>
<p>Optionally, if the input annotated file contains a tier named exactly &quot;Name&quot;, then the content of this tier will be used to fix output file names.</p>
<figure>
<img src="./etc/screenshots/ipu-seg-result3.png" alt="Data to split" /><figcaption>Data to split</figcaption>
</figure>
<p>In the example above, the automatic process will create 6 files: FLIGTH.wav, FLIGHT.txt, MOVIES.wav, MOVIES.txt, SLEEP.wav and SLEEP.txt. It is up to the user to perform another IPU segmentation of these files to get another file format than txt (xra, TextGrid, ...) thanks to the previous section &quot;Silence/Speech segmentation time-aligned with a transcription&quot;.</p>
<figure>
<img src="./etc/screenshots/ipu-seg-result3-files.png" alt="Data split" /><figcaption>Data split</figcaption>
</figure>
<h3 id="perform-ipus-segmentation-with-the-gui"><a href="#perform-ipus-segmentation-with-the-gui">Perform IPUs Segmentation with the GUI</a></h3>
<p>Click on the IPUs Segmentation activation button and on the &quot;Configure...&quot; blue text to fix options.</p>
<blockquote>
<p>Notice that all time values are indicated in seconds.</p>
</blockquote>
<h3 id="perform-ipus-segmentation-with-the-cli"><a href="#perform-ipus-segmentation-with-the-cli">Perform IPUs Segmentation with the CLI</a></h3>
<p><code>wavsplit.py</code> is the program to perform the IPU-segmentation, i.e. silence/speech segmentation. When this program is used from an audio speech sound and eventually a transcription, it consists in aligning macro-units of a document with the corresponding sound.</p>
<blockquote>
<p>Notice that all time values are indicated in seconds.</p>
</blockquote>
<pre><code>usage: wavsplit.py -w file [options]</code></pre>
<p>Generic options:</p>
<pre><code>    -w file         audio input file name
    -d delta shift  Add this time value to each start
                    bound of the IPUs
    -D delta shift  Add this time value to each end
                    bound of the IPUs
    -h, --help      show the help message and exit</code></pre>
<p>Options that can be fixed for the Speech/Silence segmentation:</p>
<pre><code>    -r float    Window size to estimate rms, in seconds
                (default value is: 0.010)
    -m value    Drop speech shorter than m seconds long
                (default value is: 0.300)
    -s value    Drop silences shorter than s seconds long
                (default value is: 0.200)
    -v value    Assume that a rms lower than v is a silence
                (default value is: 0 which means to auto-adjust)</code></pre>
<p>Options that can be fixed for the Speech/Silence segmentation with a given orthographic transcription. It must be choose one of -t or -n options:</p>
<pre><code>    -t file     Input transcription file (default: None)
    -n value    Input transcription tier number (default: None)
    -N          Adjust the volume cap until it splits
                into nb tracks (default: 0=automatic)</code></pre>
<p>Output options:</p>
<pre><code>    -o dir      Output directory name       (default: None)
    -e ext      Output tracks extension     (default: txt)
    -p file     File with the segmentation  (default: None)</code></pre>
<p>Examples of use to get each IPU in a wav file and its corresponding textgrid:</p>
<pre><code>./sppas/bin/wavsplit.py -w ./samples/samples-eng/oriana1.WAV
-d 0.01
-D 0.01
-t ./samples/samples-eng/oriana1.txt
-p ./samples/samples-eng/oriana1.xra

./sppas/bin/wavsplit.py -w ./samples/samples-eng/oriana1.WAV
-t ./samples/samples-eng/oriana1.xra
-o ./samples/samples-eng/oriana1
-e textgrid</code></pre>
<h2 id="tokenization"><a href="#tokenization">Tokenization</a></h2>
<h3 id="overview-1"><a href="#overview-1">Overview</a></h3>
<p>Tokenization is also known as &quot;Text Normalization&quot; the process of segmenting a text into tokens. In principle, any system that deals with unrestricted text need the text to be normalized. Texts contain a variety of &quot;non-standard&quot; token types such as digit sequences, words, acronyms and letter sequences in all capitals, mixed case words, abbreviations, roman numerals, URL's and e-mail addresses... Normalizing or rewriting such texts using ordinary words is then an important issue. The main steps of the text normalization implemented in SPPAS (Bigi 2011) are:</p>
<ul>
<li>Remove punctuation;</li>
<li>Lower the text;</li>
<li>Convert numbers to their written form;</li>
<li>Replace symbols by their written form, thanks to a &quot;replacement&quot; dictionary, located into the sub-directory &quot;repl&quot; in the &quot;resources&quot; directory. Do not hesitate to add new replacements in this dictionary.</li>
<li>Word segmentation based on the content of a lexicon. If the result is not corresponding to your expectations, fill free to modify the lexicon, located in the &quot;vocab&quot; sub-directory of the &quot;resources&quot; directory. The lexicon contains one word per line.</li>
</ul>
<h3 id="adapt-tokenization"><a href="#adapt-tokenization">Adapt Tokenization</a></h3>
<p>Word segmentation of SPPAS is mainly based on the use of a lexicon. If a segmentation is not as expected, it is up to the user to modify the lexicon. Lexicons of all supported languages are all located in the folder &quot;vocab&quot; of the &quot;resources&quot; directory. They are in the form of &quot;one word at a line&quot;.</p>
<h3 id="perform-tokenization-with-the-gui"><a href="#perform-tokenization-with-the-gui">Perform Tokenization with the GUI</a></h3>
<p>The SPPAS Tokenization system takes as input a file (or a list of files) for which the name strictly match the name of the audio file except the extension. For example, if a file with name &quot;oriana1.wav&quot; is given, SPPAS will search for a file with name &quot;oriana1.xra&quot; at a first stage if &quot;.xra&quot; is set as the default extension, then it will search for other supported extensions until a file is found.</p>
<p>This file must include a tier with the orthographic transcription. At a first stage, SPPAS tries to find a tier with <code>transcription</code> as name. If such a tier does not exist, SPPAS tries to find a tier that contains one of the following strings:</p>
<ol type="1">
<li><code>trans</code></li>
<li><code>trs</code></li>
<li><code>ipu</code></li>
<li><code>ortho</code></li>
<li><code>toe</code></li>
</ol>
<p>The first tier that matches is used (case insensitive search).</p>
<p>Tokenization produces a file with &quot;-tokens&quot; appended to its name, i.e. &quot;oriana1-tokens.xra&quot; for the previous example. This file is including only one tier with the resulting tokenization and with name &quot;Tokenization&quot;. In case of an Enriched Orthographic Transcription, to get both faked and standard tokenized tiers, check the corresponding option. Then, two tiers will be created:</p>
<ul>
<li>&quot;Tokens-std&quot;: the text normalization of the standard transcription,</li>
<li>&quot;Tokens-faked&quot;: the text normalization of the faked transcription.</li>
</ul>
<p>Read the &quot;Introduction&quot; of this chapter for a better understanding of the difference between &quot;standard&quot; and &quot;faked&quot; transcriptions.</p>
<figure>
<img src="./etc/figures/tokworkflow.bmp" alt="Text normalization workflow" /><figcaption>Text normalization workflow</figcaption>
</figure>
<p>To perform the text normalization process, click on the Tokenization activation button, select the language and click on the &quot;Configure...&quot; blue text to fix options.</p>
<h3 id="perform-tokenization-with-the-cli"><a href="#perform-tokenization-with-the-cli">Perform Tokenization with the CLI</a></h3>
<p><code>tokenize.py</code> is the program to perform Tokenization, i.e. the text normalization of a given file or a raw text.</p>
<pre><code>usage: tokenize.py -r vocab [options]

optional arguments:
    -r vocab         Vocabulary file name
    -i file          Input file name
    -o file          Output file name
    --delimiter char Use a delimiter character instead of a space for word delimiter.
    -h, --help       Show the help message and exit</code></pre>
<p>The following situations are possible:</p>
<ol type="1">
<li><p>no input is given: the input is <code>stdin</code> and the output is <code>stdout</code> (if an output file name is given, it is ignored). In case of Enriched Orthographic Transcription, only the faked tokenization is printed.</p></li>
<li><p>an input is given, but no output: the result of the tokenization is added to the input file.</p></li>
<li><p>an input and an output are given: the output file is created (or erased if the file already exists) and the result of the tokenization is added to this file..</p></li>
</ol>
<p>Example of use, using stdin/stdout:</p>
<pre><code>$ echo &quot;The te(xt) to normalize 123.&quot; |\
  ./sppas/bin/tokenize.py
  -r ./resources/vocab/eng.vocab
$ the te to normalize one_hundred_twenty-three</code></pre>
<p>In that case, the elision mentionned with the parenthesis is removed and the number is converted to its written form. The character &quot;_&quot; is used for compound words (it replaces the whitespace).</p>
<p>Example of use on a transcribed file:</p>
<pre><code>$ ./sppas/bin/tokenize.py -r ./resources/vocab/eng.vocab
  -i ./samples/samples-eng/oriana1.xra
  -o ./samples/samples-eng/oriana1-token.xra</code></pre>
<h2 id="phonetization"><a href="#phonetization">Phonetization</a></h2>
<h3 id="overview-2"><a href="#overview-2">Overview</a></h3>
<p>Phonetization, also called grapheme-phoneme conversion, is the process of representing sounds with phonetic signs. However, converting from written text into actual sounds, for any language, cause several problems that have their origins in the relative lack of correspondence between the spelling of the lexical items and their sound contents.</p>
<p>SPPAS implements a dictionary based-solution which consists in storing a maximum of phonological knowledge in a lexicon. In this sense, this approach is language-independent. SPPAS phonetization process is the equivalent of a sequence of dictionary look-ups.</p>
<p>Actually, some words can correspond to several entries in the dictionary with various pronunciations. These pronunciation variants are stored in the phonetization result. By convention, whitespace separate words, minus characters separate phones and pipe character separate phonetic variants of a word. For example, the transcription utterance:</p>
<ul>
<li>Transcription: <code>The flight was 12 hours long.</code></li>
<li>Tokenization: <code>the flight was twelve hours long</code></li>
<li>Phonetization: <code>D-@|D-V|D-i: f-l-aI-t w-A-z|w-V-z|w-@-z|w-O:-z t-w-E-l-v aU-3:r-z|aU-r-z l-O:-N</code></li>
</ul>
<p>Many of the other systems assume that all words of the speech transcription are mentioned in the pronunciation dictionary. On the contrary, SPPAS includes a language-independent algorithm which is able to phonetize unknown words of any language as long as a (minimum) dictionary is available (Bigi 2013)! If such case occurs during the phonetization process, a WARNING indicates it in the Procedure Outcome Report.</p>
<p>Since the phonetization is only based on the use of a pronunciation dictionary, the quality of such a phonetization only depends on this resource.</p>
<h3 id="adapt-phonetization"><a href="#adapt-phonetization">Adapt Phonetization</a></h3>
<p>If a pronunciation is not as expected, it is up to the user to change it in the dictionary. All dictionaries are located in the folder &quot;dict&quot; of the &quot;resources&quot; directory.</p>
<p>SPPAS uses the HTK ASCII format for dictionaries. As example, below is a piece of the <code>eng.dict</code> file:</p>
<pre><code>    THE             [THE]           D @
    THE(2)          [THE]           D V
    THE(3)          [THE]           D i:
    THEA            [THEA]          T i: @
    THEALL          [THEALL]        T i: l
    THEANO          [THEANO]        T i: n @U
    THEATER         [THEATER]       T i: @ 4 3:r
    THEATER&#39;S       [THEATER&#39;S]     T i: @ 4 3:r z</code></pre>
<p>The first column indicates the word, followed by the variant number (except for the first one). The second column indicates the word between brackets; however brackets can also be empty. The last columns are the succession of phones, separated by a whitespace.</p>
<p>SPPAS dictionaries are mainly based on X-SAMPA international standard for phoneme encoding. See the chapter &quot;Resources&quot; of this documentation to know the list of accepted phones for a given language. This list can't be extended nor modified while using SPPAS. However, it can be improved from one version to the other one thanks to user comments (to be sent to the author).</p>
<h3 id="perform-phonetization-with-the-gui"><a href="#perform-phonetization-with-the-gui">Perform Phonetization with the GUI</a></h3>
<p>The Phonetization process takes as input a file that strictly match the audio file name except for the extension and that &quot;-tokens&quot; is appended. For example, if the audio file name is &quot;oriana1.wav&quot;, the expected input file name is &quot;oriana1-tokens.xra&quot; if .xra is the default extension for annotations. This file must include a <strong>normalized</strong> orthographic transcription. The name of such tier must contains one of the following strings:</p>
<ol type="1">
<li>&quot;tok&quot;</li>
<li>&quot;trans&quot;</li>
</ol>
<p>The first tier that matches one of these requirements is used (this match is case insensitive).</p>
<p>Phonetization produces a file with &quot;-phon&quot; appended to its name, i.e. &quot;oriana1-phon.xra&quot; for the previous example. This file is including only one tier with the resulting phonetization and with name &quot;Phonetization&quot;.</p>
<figure>
<img src="./etc/figures/phonworkflow.bmp" alt="Phonetization workflow" /><figcaption>Phonetization workflow</figcaption>
</figure>
<p>To perform the grapheme-to-phoneme convertion, click on the Phonetization activation button, select the language and click on the &quot;Configure...&quot; blue text to fix options.</p>
<h3 id="perform-phonetization-with-the-cli"><a href="#perform-phonetization-with-the-cli">Perform Phonetization with the CLI</a></h3>
<p><code>phonetize.py</code> is the program performs Phonetization, i.e. grapheme to phoneme conversion on a given file.</p>
<pre><code>usage: phonetize.py -r dict [options]

optional arguments:
    -r dict      Pronunciation dictionary file name (HTK-ASCII format)
    -m map       Pronunciation mapping table
    -i file      Input file name
    -o file      Output file name
    --nounk      Disable unknown word phonetization
    -h, --help   Show the help message and exit</code></pre>
<p>Examples of use:</p>
<pre><code>$ echo &quot;the te totu&quot; |\
 ./sppas/bin/phonetize.py
    -r ./resources/dict/eng.dict
    --nounk
$ D-@|D-V|D-i: t-i: UNK
$
$ echo &quot;the te totu&quot; |\
 ./sppas/bin/phonetize.py -r ./resources/dict/eng.dict
$ D-@|D-V|D-i: t-i: t-u-t-u|t-i-t-u|t-A-t-u|t-@-t-u</code></pre>
<p>If we suppose that the previous text was read by a French native speaker, the previous example can be phonetized as:</p>
<pre><code>$ echo &quot;the te totu&quot; |\
 ./sppas/bin/phonetize.py
     -r ./resources/dict/eng.dict
     -m ./resources/dict/eng-fra.map
$ D-@|z-@|v-@|z-9|D-V|v-9|v-V|D-9|z-V|D-i:|z-i|v-i|D-i|v-i:|z-i:
  t-i:|t-i
  t-u-t-u|t-i-t-u|t-A-t-u|t-@-t-u</code></pre>
<p>Example of use on a tokenized file:</p>
<pre><code>./sppas/bin/phonetize.py
-d ./resources/dict/eng.dict
-i ./samples/samples-eng/oriana1-token.xra
-o ./samples/samples-eng/oriana1-phon.xra</code></pre>
<h2 id="alignment"><a href="#alignment">Alignment</a></h2>
<h3 id="overview-3"><a href="#overview-3">Overview</a></h3>
<p>Alignment, also called phonetic segmentation, is the process of aligning speech with its corresponding transcription at the phone level. The alignment problem consists in a time-matching between a given speech unit along with a phonetic representation of the unit.</p>
<p>SPPAS is based on the Julius Speech Recognition Engine (SRE). Speech Alignment also requires an Acoustic Model in order to align speech. An acoustic model is a file that contains statistical representations of each of the distinct sounds of one language. Each phoneme is represented by one of these statistical representations.</p>
<p>Speech segmentation was evaluated for French: in average, automatic speech segmentation is 95% of times within 40ms compared to the manual segmentation and was tested on read speech and on conversational speech (Bigi 2014).</p>
<figure>
<img src="./etc/screenshots/alignment.png" alt="SPPAS alignment output example" /><figcaption>SPPAS alignment output example</figcaption>
</figure>
<h3 id="adapt-alignment"><a href="#adapt-alignment">Adapt Alignment</a></h3>
<p>For Speech segmentation, the better Acoustic Model, the better results. In order to get a better result, any user can append or replace the models included in SPPAS in the &quot;models&quot; folder of the &quot;resources&quot; directory. SPPAS only supports HTK-ASCII acoustic models, trained from 16 bits 16000 Hz WAVE files. The models can be improved if they are re-trained with more data. So, any new data is welcome (send an e-mail to the author).</p>
<h3 id="perform-alignment-with-the-gui"><a href="#perform-alignment-with-the-gui">Perform Alignment with the GUI</a></h3>
<p>The Alignment process takes as input one or two files that strictly match the audio file name except for the extension and that &quot;-phon&quot; is appended for the first one and &quot;-tokens&quot; for the optionnal second one. For example, if the audio file name is &quot;oriana1.wav&quot;, the expected input file name is &quot;oriana1-phon.xra&quot; with phonetization and optionnally &quot;oriana1-tokens.xra&quot; with tokenization, if .xra is the default extension for annotations.</p>
<p>The speech segmentation process provides one file with name &quot;-palign&quot; appended to its name, i.e. &quot;oriana1-palign.xra&quot; for the previous example. This file includes one or two tiers:</p>
<ul>
<li>&quot;PhonAlign&quot; is the segmentation at the phone level;</li>
<li>&quot;TokensAlign&quot; is the segmentation at the word level (if a file with tokenization was found).</li>
</ul>
<figure>
<img src="./etc/figures/alignworkflow.bmp" alt="Alignment workflow" /><figcaption>Alignment workflow</figcaption>
</figure>
<p>The following options are available to configure Alignment:</p>
<ul>
<li>choose the speech segmentation system. It can be either: julius, hvite or basic</li>
<li>perform basic alignment if the aligner failed, instead such intervals are empty.</li>
<li>remove working directory will keep only alignment result: it will remove working files. Working directory includes one wav file per unit and a set of text files per unit.</li>
<li>create the Activity tier will append another tier with activities as intervals, i.e. speech, silences, laughter, noises...</li>
<li>create the PhnTokAlign will append anoter tier with intervals of the phonetization of each word.</li>
</ul>
<p>To perform speech segmentation, click on the Alignment activation button, select the language and click on the &quot;Configure...&quot; blue text to fix options.</p>
<h3 id="perform-alignment-with-the-cli"><a href="#perform-alignment-with-the-cli">Perform Alignment with the CLI</a></h3>
<p><code>alignment.py</code> is the program to perform automatic speech segmentation of a given file.</p>
<pre><code>usage: alignment.py -w file -i file -r file -o file [options]

optional arguments:
    -w file     Input wav file name
    -i file     Input file name with the phonetization
    -I file     Input file name with the tokenization
    -r file     Directory of the acoustic model of the language of the text
    -R file     Directory of the acoustic model of the mother language
                of the speaker
    -o file     Output file name with alignments
    -a name     Aligner name. One of: julius, hvite, basic (default: julius)
    --extend    Extend last phoneme/token to the wav duration
    --basic     Perform a basic alignment if error with the aligner
    --infersp   Add &#39;sp&#39; at the end of each token and let the aligner
                to decide the relevance
    --noclean   Do not remove temporary data
    -h, --help  Show the help message and exit</code></pre>
<p>Example of use:</p>
<pre><code>./sppas/bin/alignment.py
-r ./resources/models/models-eng
-w ./samples/samples-eng/oriana1.WAV
-i ./samples/samples-eng/oriana1-phon.xra
-I ./samples/samples-eng/oriana1-token.xra
-o ./samples/samples-eng/oriana1-palign.xra</code></pre>
<h2 id="syllabification"><a href="#syllabification">Syllabification</a></h2>
<h3 id="overview-4"><a href="#overview-4">Overview</a></h3>
<p>The syllabification of phonemes is performed with a rule-based system from time-aligned phonemes. This phoneme-to-syllable segmentation system is based on 2 main principles:</p>
<ul>
<li>a syllable contains a vowel, and only one;</li>
<li>a pause is a syllable boundary.</li>
</ul>
<p>These two principles focus the problem of the task of finding a syllabic boundary between two vowels. As in state-of-the-art systems, phonemes were grouped into classes and rules established to deal with these classes. We defined general rules followed by a small number of exceptions. Consequently, the identification of relevant classes is important for such a system.</p>
<p>We propose the following classes, for both French and Italian set of rules:</p>
<ul>
<li>V - Vowels,</li>
<li>G - Glides,</li>
<li>L - Liquids,</li>
<li>O - Occlusives,</li>
<li>F - Fricatives,</li>
<li>N - Nasals.</li>
</ul>
<figure>
<img src="./etc/screenshots/syll-example.png" alt="Syllabification example" /><figcaption>Syllabification example</figcaption>
</figure>
<p>The rules we propose follow usual phonological statements for most of the corpus. A configuration file indicates phonemes, classes and rules. This file can be edited and modified to adapt the syllabification (Bigi et al. 2010).</p>
<h3 id="adapt-syllabification"><a href="#adapt-syllabification">Adapt Syllabification</a></h3>
<p>If the syllabification is not as expected, any user can change the set of rules. The configuration file is located in the folder &quot;syll&quot; of the &quot;resources&quot; directory. The syllable configuration file is a simple ASCII text file that can be edited with Notepad++ (Windows) or TextEdit (MacOS) or any other text editor.</p>
<p>At first, the list of phonemes and the class symbol associated with each of the phonemes are described as, for example:</p>
<ul>
<li><code>PHONCLASS e V</code></li>
<li><code>PHONCLASS p O</code></li>
</ul>
<p>The couples phoneme/class are made of 3 columns: the first column is the key-word PHONCLASS, the second column is the phoneme symbol, the third column is the class symbol.The constraints on this definition are:</p>
<ul>
<li>a pause is mentioned with the class-symbol #,</li>
<li>a class-symbol is only one upper-case character, except:
<ul>
<li>the character X if forbidden;</li>
<li>the characters V and W are used for vowels.</li>
</ul></li>
</ul>
<p>The second part of the configuration file contains the rules. The first column is a keyword, the second column describes the classes between two vowels and the third column is the boundary location. The first column can be:</p>
<ul>
<li><code>GENRULE</code>,</li>
<li><code>EXCRULE</code>, or</li>
<li><code>OTHRULE</code>.</li>
</ul>
<p>In the third column, a 0 means the boundary is just after the first vowel, 1 means the boundary is one phoneme after the first vowel, etc. Here are some examples, corresponding to the rules described in this paper for spontaneous French:</p>
<ul>
<li><code>GENRULE VXV 0</code></li>
<li><code>GENRULE VXXV 1</code></li>
<li><code>EXCRULE VFLV 0</code></li>
<li><code>EXCRULE VOLGV 0</code></li>
</ul>
<p>Finally, to adapt the rules to specific situations that the rules failed to model, we introduced some phoneme sequences and the boundary definition. Specific rules contain only phonemes or the symbol &quot;ANY&quot; which means any phoneme. It consists of 7 columns: the first one is the key-word OTHRULE, the 5 following columns are a phoneme sequence where the boundary should be applied to the third one by the rules, the last column is the shift to apply to this boundary. In the following example:</p>
<p><code>OTHRULE ANY ANY p s k -2</code></p>
<h3 id="perform-syllabification-with-the-gui"><a href="#perform-syllabification-with-the-gui">Perform Syllabification with the GUI</a></h3>
<p>The Syllabification process takes as input a file that strictly match the audio file name except for the extension and that &quot;-palign&quot; is appended. For example, if the audio file name is &quot;oriana1.wav&quot;, the expected input file name is &quot;oriana1-palign.xra&quot; if .xra is the default extension for annotations. This file must include a tier containing the time-aligned phonemes with name &quot;PhonAlign&quot;.</p>
<p>The annotation provides an annotated file with &quot;-salign&quot; appended to its name, i.e. &quot;oriana1-salign.xra&quot; for the previous example. This file is including 3 tiers: Syllables, Classes and Structures.</p>
<figure>
<img src="./etc/figures/syllworkflow.bmp" alt="Syllabification workflow" /><figcaption>Syllabification workflow</figcaption>
</figure>
<p>Click on the Syllabification activation button, select the language and click on the &quot;Configure...&quot; blue text to fix options.</p>
<h3 id="perform-syllabification-with-the-cli"><a href="#perform-syllabification-with-the-cli">Perform Syllabification with the CLI</a></h3>
<p><code>syllabify.py</code> is the program performs automatic syllabification of a given file with time-aligned phones.</p>
<pre><code>usage: syllabify.py -r config [options]

optional arguments:
    -r config   Rules configuration file name
    -i file     Input file name (time-aligned phonemes)
    -o file     Output file name
    -e file     Reference file name to syllabify between intervals
    -t string   Reference tier name to syllabify between intervals
    --nophn     Disable the output of the result that does not use the reference  tier
    -h, --help   Show the help message and exit</code></pre>
<h2 id="repetitions"><a href="#repetitions">Repetitions</a></h2>
<h3 id="overview-5"><a href="#overview-5">Overview</a></h3>
<p>This automatic detection focus on word repetitions, which can be an exact repetition (named strict echo) or a repetition with variation (named non-strict echo).</p>
<p>SPPAS implements <em>self-repetitions</em> and <em>other-repetitions</em> detection (Bigi et al. 2014). The system is based only on lexical criteria. The proposed algorithm is focusing on the detection of the source.</p>
<p>The Graphical User Interface only allows to detect self-repetitions. Use the Command-Line User Interface if you want to get other-repetitions.</p>
<p>This process requires a list of stop-words, and a dictionary with lemmas (the system can process without it, but the result is better with it). Both lexicons are located in the &quot;vocab&quot; folder of the &quot;resources&quot; directory.</p>
<h3 id="perform-repetitions-with-the-gui"><a href="#perform-repetitions-with-the-gui">Perform Repetitions with the GUI</a></h3>
<p>The automatic annotation takes as input a file with (at least) one tier containing the time-aligned tokens of the speaker (and another file/tier for other-repetitions). The annotation provides one annotated file with 2 tiers: Sources and Repetitions.</p>
<figure>
<img src="./etc/figures/repetworkflow.bmp" alt="Repetition detection workflow" /><figcaption>Repetition detection workflow</figcaption>
</figure>
<p>Click on the Self-Repetitions activation button, select the language and click on the &quot;Configure...&quot; blue text to fix options.</p>
<h3 id="perform-repetitions-with-the-cli"><a href="#perform-repetitions-with-the-cli">Perform Repetitions with the CLI</a></h3>
<p><code>repetition.py</code> is the program to perform automatic detection of self-repetitions or other-repetitions if a second speaker is given.</p>
<p>It can be language-dependent (better results) or language-independent.</p>
<pre><code>usage: repetition.py -i file [options]

optional arguments:
  -h, --help  show this help message and exit
  -i file     Input file name with time-aligned tokens of the self-speaker
  -r folder   Directory with resources
  -l lang     Language code in iso639-3
  -I file     Input file name with time-aligned tokens of the echoing-speaker
              (if ORs)
  -o file     Output file name</code></pre>
<h2 id="momel-and-intsint"><a href="#momel-and-intsint">Momel and INTSINT</a></h2>
<h3 id="momel-modelling-melody"><a href="#momel-modelling-melody">Momel (modelling melody)</a></h3>
<p>Momel is an algorithm for the automatic modelling of fundamental frequency (F0) curves using a technique called assymetric modal quaratic regression.</p>
<p>This technique makes it possible by an appropriate choice of parameters to factor an F0 curve into two components:</p>
<ul>
<li>a macroprosodic component represented by a a quadratic spline function defined by a sequence of target points &lt; ms, hz &gt;.</li>
<li>a microprosodic component represented by the ratio of each point on the F0 curve to the corresponding point on the quadratic spline function.</li>
</ul>
<p>The algorithm which we call Asymmetrical Modal Regression comprises the following four stages:</p>
<p>For details, see the following reference:</p>
<blockquote>
<p><strong>Daniel Hirst and Robert Espesser</strong> (1993). <em>Automatic modelling of fundamental frequency using a quadratic spline function.</em> Travaux de lInstitut de Phontique dAix. vol. 15, pages 71-85.</p>
</blockquote>
<p>The SPPAS implementation of Momel requires a file with the F0 values. It must be <strong>sampled at 10 ms</strong>. Two extensions are supported:</p>
<ul>
<li>.PitchTier, from Praat.</li>
<li>.hz, from any tool, is a file with one value per line.</li>
</ul>
<p>The following options can be fixed:</p>
<ul>
<li>Window length used in the &quot;cible&quot; method</li>
<li>F0 threshold: Maximum F0 value</li>
<li>F0 ceiling: Minimum F0 value</li>
<li>Maximum error: Acceptable ratio between two F0 values</li>
<li>Window length used in the &quot;reduc&quot; method</li>
<li>Minimal distance</li>
<li>Minimal frequency ratio</li>
<li>Eliminate glitch option: Filter f0 values before 'cible'</li>
</ul>
<h3 id="encoding-of-f0-target-points-using-the-intsint-system"><a href="#encoding-of-f0-target-points-using-the-intsint-system">Encoding of F0 target points using the &quot;INTSINT&quot; system</a></h3>
<p>INTSINT assumes that pitch patterns can be adequately described using a limited set of tonal symbols, T,M,B,H,S,L,U,D (standing for : Top, Mid, Bottom, Higher, Same, Lower, Up-stepped, Down-stepped respectively) each one of which characterises a point on the fundamental frequency curve.</p>
<p>The rationale behind the INTSINT system is that the F0 values of pitch targets are programmed in one of two ways : either as absolute tones T, M, B which are assumed to refer to the speakers overall pitch range (within the current Intonation Unit), or as relative tones H, S, L, U, D assumed to refer only to the value of the preceding target point.</p>
<figure>
<img src="./etc/img/INTSINT-tones.png" alt="INTSINT example" /><figcaption>INTSINT example</figcaption>
</figure>
<p>The rationale behind the INTSINT system is that the F0 values of pitch targets are programmed in one of two ways : either as absolute tones T, M, B which are assumed to refer to the speakers overall pitch range (within the current Intonation Unit), or as relative tones H, S, L, U, D assumed to refer only to the value of the preceding target point.</p>
<p>A distinction is made between non-iterative H, S, L and iterative U, D relative tones since in a number of descriptions it appears that iterative raising or lowering uses a smaller F0 interval than non-iterative raising or lowering. It is further assumed that the tone S has no iterative equivalent since there would be no means of deciding where intermediate tones are located.</p>
<figure>
<img src="./etc/screenshots/Momel-INTSINT.png" />
</figure>
<blockquote>
<p><strong>D.-J. Hirst</strong> (2011). <em>The analysis by synthesis of speech melody: from data to models</em>, Journal of Speech Sciences, vol. 1(1), pages 55-83.</p>
</blockquote>
<h3 id="perform-momel-and-intsint-with-the-gui"><a href="#perform-momel-and-intsint-with-the-gui">Perform Momel and INTSINT with the GUI</a></h3>
<p>Click on the Momel activation button, select the language and click on the &quot;Configure...&quot; blue text to fix options. And click on the INTSINT activation button.</p>
<h3 id="perform-momel-and-intsint-with-the-cli"><a href="#perform-momel-and-intsint-with-the-cli">Perform Momel and INTSINT with the CLI</a></h3>
<p><code>momel-intsint.py</code> is the program perform both momel and INTSINT.</p>
<pre><code>usage: momel-intsint.py [options] -i file

optional arguments:
  -i file            Input file name (extension: .hz or .PitchTier)
  -o file            Output file name (default: stdout)
  --win1 WIN1        Target window length (default: 30)
  --lo LO            f0 threshold (default: 50)
  --hi HI            f0 ceiling (default: 600)
  --maxerr MAXERR    Maximum error (default: 1.04)
  --win2 WIN2        Reduct window length (default: 20)
  --mind MIND        Minimal distance (default: 5)
  --minr MINR        Minimal frequency ratio (default: 0.05)
  --non-elim-glitch
  -h, --help         Show the help message and exit</code></pre>
<hr />
    <!----------------------------------------------------------------->

    <div class="footer">
        <a href="index.html"><img src="./etc/logos/sppas-logo.png" alt="SPPAS"></a>
        <p class="copyright">
            <a href="mailto:brigitte.bigi@gmail.com">Brigitte Bigi</a> &copy; 2011-2016
        </p>
    </div>

    <!----------------------------------------------------------------->

    <span class="scrollT"></span>
    <script type="text/javascript" src="./etc/scripts/jquery.min.js"></script>
    <script type="text/javascript" src="./etc/scripts/scrollT.js"></script>
    <script type="text/javascript">
        // On active la fonction permettant de faire apparaitre l'indicateur 
        // de scroll pour remonter en haut de page.
        $(document).ready(function() 
        {
            $(this).scrollT();
        });
    </script>

    <!----------------------------------------------------------------->
</body>
</html>
